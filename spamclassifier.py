# -*- coding: utf-8 -*-
"""spamclassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14mu86f_kajFnIyjvcqWZ7X-mjmsJw5SF
"""

#importing the necessary libraries
import pandas as pd
import numpy as np

#printing the csv file for training the model
data = pd.read_csv("spam.csv", encoding='ISO-8859-1')
data.head(5)

data.columns
data.info()

data.isnull().sum() #checking it's null value is there or not

#applying map function to replace ham with 0 and spam with 1
data['Category']=data['Category'].map({'ham':0,'spam':1})
data.head()

#analyzing the no of spam and ham
import matplotlib.pyplot as plt
plt.pie(data['Category'].value_counts(),labels=['ham','spam'],autopct="%0.2f")
plt.show()

data['num_char']=data['Message'].apply(len)
data.head(5)

import nltk
nltk.download('punkt_tab')

data['num_words']=data['Message'].apply(lambda x:len(nltk.word_tokenize(x)))
data.head(5)

data['num_sentence']=data['Message'].apply(lambda x:len(nltk.sent_tokenize(x)))
data.head(3)
data.columns

#Ham Messages description
data[data['Category']==0][['num_char', 'num_words', 'num_sentence']].describe()

#spam Messages description
data[data['Category']==1][['num_char', 'num_words', 'num_sentence']].describe()

import seaborn as sns  #importing seaborn library for visualization
sns.histplot(data[data['Category']==0]['num_char'])
sns.histplot(data[data['Category']==1]['num_char'],color='red')
plt.show()

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords.words('english')

import string
string.punctuation

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()
ps.stem('running')

def transform(Message):
  Message=Message.lower()
  Message=nltk.word_tokenize(Message)

  y=[]
  for i in Message:
    if i.isalnum():
      y.append(i)

  Message=y[:]
  y.clear()

  for i in Message:
    if i not in stopwords.words('english') and string.punctuation:
      y.append(i)

  Message=y[:]
  y.clear()
  for i in Message:
    ds=ps.stem(i)
    y.append(ds)
  return " ".join(y)

data['transformed_text']=data['Message'].apply(transform)
data.head(5)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc = WordCloud(height=400,width=400, background_color='white', min_font_size=10)

spam_wc=wc.generate(data[data['Category']==1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spam_wc)

ham_wc=wc.generate(data[data['Category']==0]['transformed_text'].str.cat(sep=" "))
plt.imshow(ham_wc)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

X=data['Message']
y=data['Category']
X.shape
y.shape

cv=CountVectorizer()

X=cv.fit_transform(X)

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

from sklearn.naive_bayes import MultinomialNB
model=MultinomialNB()
model.fit(x_train,y_train)
model.score(x_test,y_test)*100

msg="You won an 1cr at lucky draw"
data=[msg]
vect=cv.transform(data).toarray()
my_prediction=model.predict(vect)

import pickle
pickle.dump(model,open('spam123.pkl','wb'))
pickle.dump(cv,open('vect123.pkl','wb'))